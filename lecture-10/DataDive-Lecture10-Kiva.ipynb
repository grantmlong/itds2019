{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Dive Week 10: Fitting and Evaluating Classification Models\n",
    "\n",
    "Today we'll be looking at a real world application of logistic regression using data from loans requests posted on [Kiva.org](https://www.kiva.org/). \n",
    "\n",
    "![alt text](https://www-kiva-org.global.ssl.fastly.net/cms/sites/default/files/kivablog/preview_logo_1.jpg)\n",
    "\n",
    "Kiva is an international nonprofit, founded in 2005 and based in San Francisco, with a mission to connect people through lending to alleviate poverty. Kiva seeks to celebrate and support people looking to create a better future for themselves, their families and their communities.\n",
    "\n",
    "*By lending as little as $25 on Kiva, anyone can help a borrower start or grow a business, go to school, access clean energy or realize their potential. For some, it’s a matter of survival, for others it’s the fuel for a life-long ambition.*\n",
    "\n",
    "## Today's Modeling Objective\n",
    "Our focus today will be determining whether microfinance projects on [Kiva's site](https://www.kiva.org/) receive funding or not using a host of features made available by Kiva, along with some features we'll design ourselves. \n",
    "\n",
    "Today's data is a subsample of projects in Kenya, one of Kiva's most active countries for lending. We'll be working with 18,000 observation, 3,000 or which were funded. Documentation on the data is available on [Kiva's website](http://build.kiva.org/docs/data/basic_types).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 40)\n",
    "random_state = 20180415"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://grantmlong.com/data/kiva_kenya_sample.csv')\n",
    "print(df.shape)\n",
    "print(list(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Familiarizing Ourselves with the Data\n",
    "\n",
    "### To get a sense of the different data points and examples here:\n",
    " 1. Print **three** randomly selected project descriptions.\n",
    " 2. Print **five** randomly selected examples of `POSTED_TIME`.\n",
    " 3. Print the unique values and counts for `ACTIVITY_NAME`.\n",
    " 4. Print the unique values and counts for `SECTOR_NAME`.\n",
    " 5. Summarize the values for loan amount.\n",
    " 6. Create a histogram for `NUM_LENDERS_TOTAL`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print three randomly selected project descriptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print five randomly selected examples of POSTED_TIME.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the unique values and counts for ACTIVITY_NAME.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the unique values and counts for SECTOR_NAME.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the values for loan amount.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram for NUM_LENDERS_TOTAL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidying Up Our Data\n",
    "\n",
    "Now that we've taken a closer look at our data, there are a few things we'll want to do to prep our data for modeling: \n",
    " 1. Create a target variable\n",
    " 2. Generate a more usable version of the `POSTED_TIME` column.\n",
    " 3. Generate a variable with the amount of planned time before expiration for each project. \n",
    " 4. Generate boolean variables for each of the following categorical columns:\n",
    "   * `SECTOR_NAME`\n",
    "   * `ACTIVITY_NAME`\n",
    "   * `REPAYMENT_INTERVAL`\n",
    " 5. Create boolean variables for a handful of other free text and categorical columns, including:\n",
    "   * `BORROWER_PICTURED`\n",
    "   * `BORROWER_GENDERS`\n",
    "   * `DISTRIBUTION_MODEL`\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's choose and create a variable for our target. \n",
    "Remember, we're trying to determine whether microfinance projects on Kiva's site receive funding or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.STATUS.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['success'] = (df.STATUS=='funded')*1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's make features for both `posted_year` and `posted_duration` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['posted_year'] = pd.to_datetime(df.POSTED_TIME).dt.year\n",
    "df['posted_duration'] = (pd.to_datetime(df.PLANNED_EXPIRATION_TIME)\n",
    "                             - pd.to_datetime(df.POSTED_TIME)\n",
    "                            ).dt.days\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's create binary (1/0) features for `SECTOR_NAME`, `ACTIVITY_NAME`, and `REPAYMENT_INTERVAL`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_names = ['sector_' + name.lower() for name in df.SECTOR_NAME.unique()]\n",
    "for name in df.SECTOR_NAME.unique():\n",
    "    df['sector_' + name.lower()] = (df.SECTOR_NAME==name)*1\n",
    "\n",
    "activity_names = ['activity_' + name.lower() for name in df.ACTIVITY_NAME.unique()]\n",
    "for name in df.ACTIVITY_NAME.unique():\n",
    "    df['activity_' + name.lower()] = (df.ACTIVITY_NAME==name)*1\n",
    "\n",
    "repayment_types = ['repayment_' + interval.lower() for interval in df.REPAYMENT_INTERVAL.unique()]\n",
    "for interval in df.REPAYMENT_INTERVAL.unique():\n",
    "    df['repayment_' + interval.lower()] = (df.REPAYMENT_INTERVAL==interval)*1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's create binary (1/0) features for `has_borrower_pic`, `has_female_borrower`, `direct_distribution`, and `currency_usd`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_names = ['has_borrower_pic', 'has_female_borrower', 'direct_distribution', 'currency_usd']\n",
    "\n",
    "df['has_borrower_pic'] = df.BORROWER_PICTURED.str.contains('true')*1\n",
    "df['has_borrower_pic'] = df['has_borrower_pic'].fillna(0)\n",
    "\n",
    "df['has_female_borrower'] = df.BORROWER_GENDERS.str.contains('female')*1\n",
    "df['has_female_borrower'] = df['has_female_borrower'].fillna(0)\n",
    "\n",
    "df['direct_distribution'] = (df.DISTRIBUTION_MODEL=='direct')*1\n",
    "\n",
    "df['currency_usd'] = (df.CURRENCY=='USD')*1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Handling\n",
    "\n",
    "#### To get our data into formats that work well with `sci-kit learn`, we'll need to: \n",
    " 1. Identify our features and isolate them in a new dataframe.\n",
    " 2. Split our dataset into train and holdout splits. \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = (['posted_year', 'posted_duration', 'LOAN_AMOUNT', 'LENDER_TERM', 'NUM_JOURNAL_ENTRIES'] \n",
    "            + repayment_types + sector_names + activity_names \n",
    "            + other_names\n",
    "           )\n",
    "\n",
    "model_df = df[(features + ['success'])].dropna().reset_index()\n",
    "\n",
    "train_df, holdout_df, y_train, y_holdout = train_test_split(\n",
    "    model_df[features], \n",
    "    model_df['success'], test_size=0.1,\n",
    "    random_state=random_state)\n",
    "\n",
    "train_df['success'] = y_train\n",
    "holdout_df['success'] = y_holdout\n",
    "\n",
    "train_df.reset_index(inplace=True)\n",
    "holdout_df.reset_index(inplace=True)\n",
    "\n",
    "print(train_df.shape[0], train_df.success.mean())\n",
    "print(holdout_df.shape[0], holdout_df.success.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Let's take a quick look at all of our classification model options using cross validation. For the tree based models, we'll use the hyperparameter `max_depth=5` as a naive attempt at avoiding overfitting before we dig deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's fit and score the model, this time using cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits=5, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_results(classifier):\n",
    "    \n",
    "    results = []\n",
    "    for train, test in k_fold.split(train_df):\n",
    "        classifier.fit(train_df.loc[train, features], train_df.loc[train, 'success'])\n",
    "        y_predicted = classifier.predict(train_df.loc[test, features])\n",
    "        accuracy = accuracy_score(train_df.loc[test, 'success'], y_predicted)\n",
    "        results.append(accuracy)\n",
    "    \n",
    "    return np.mean(results), np.std(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, let's look at the cv performance of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(\n",
    "    random_state=random_state, \n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "get_cv_results(logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, let's take a naive attempt to beat Logistic Regression using a Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(\n",
    "    random_state=random_state, \n",
    "    max_depth=5\n",
    ")\n",
    "\n",
    "get_cv_results(dtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looks like a non-linear method gives us a big boost. Let's try a Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rforest = RandomForestClassifier(\n",
    "    random_state=random_state, \n",
    "    max_depth=5,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "get_cv_results(rforest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, let's take a naive approach to Gradient Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = GradientBoostingClassifier(\n",
    "    random_state=random_state, \n",
    "    max_depth=5,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "get_cv_results(gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curves, Revisited\n",
    "\n",
    "Now that we're working with a more complex data set, we should be able to build more robust learning curves than we had experience with when we were using the  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_values = range(1,50, 2)\n",
    "all_mu = []\n",
    "all_sigma = []\n",
    "\n",
    "for m in hp_values:\n",
    "\n",
    "    dtree=DecisionTreeClassifier(\n",
    "        criterion='entropy', \n",
    "        random_state=random_state, \n",
    "        max_depth=m,\n",
    "    )\n",
    "\n",
    "    mu, sigma = get_cv_results(dtree)\n",
    "    all_mu.append(mu)\n",
    "    all_sigma.append(sigma)\n",
    "    \n",
    "    print(m, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(hp_values, all_mu)\n",
    "plt.ylabel('Cross Validation Accuracy')\n",
    "plt.xlabel('Max Depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(hp_values, all_sigma)\n",
    "plt.ylabel('Cross Validation Std Dev.')\n",
    "plt.xlabel('Max Depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Model Performance\n",
    "\n",
    "We can use ROC curves to look at how our models perform across a variety of threshholds against our holdout data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(classifier, label, color):\n",
    "\n",
    "    classifier.fit(train_df[features], train_df['success'])\n",
    "    y_prob = classifier.predict_proba(holdout_df[features])\n",
    "    \n",
    "    fpr, tpr, thresh = roc_curve(holdout_df['success'], y_prob[:,1])\n",
    "    plt.plot(fpr, tpr,\n",
    "             label=label,\n",
    "             color=color, linewidth=3)\n",
    "\n",
    "    auc = roc_auc_score(holdout_df['success'], y_prob[:,1])\n",
    "    \n",
    "    print('AUC: %0.3f (%s)' % (auc, label))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = plt.figure(figsize=(14,6))\n",
    "\n",
    "logreg = LogisticRegression(\n",
    "    random_state=random_state, \n",
    "    solver='lbfgs'\n",
    ")\n",
    "plot_roc(logreg, 'Logistic Regression', 'green')\n",
    "\n",
    "dtree = DecisionTreeClassifier(\n",
    "    random_state=random_state, \n",
    "    max_depth=5\n",
    ")\n",
    "plot_roc(dtree, 'Decision Tree', 'red')\n",
    "\n",
    "rforest = RandomForestClassifier(\n",
    "    random_state=random_state, \n",
    "    max_depth=5,\n",
    "    n_estimators=100\n",
    ")\n",
    "plot_roc(rforest, 'Random Forest', 'blue')\n",
    "\n",
    "gbm = GradientBoostingClassifier(\n",
    "    random_state=random_state, \n",
    "    max_depth=5,\n",
    "    n_estimators=100\n",
    ")\n",
    "plot_roc(gbm, 'GBM', 'lightblue')\n",
    "\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review and Discussion: \n",
    " * Why do we think GBMs exhibit the best performance here? \n",
    " * In what situations might they not work as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
