{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "667ba18c-b26a-46cd-975b-e3f995db69ec"
    }
   },
   "source": [
    "# Data Dive 2: Loading and Summarizing Data\n",
    "### Overtime: Scraping the Web for Unique Data\n",
    "\n",
    "#### [Web scraping](https://en.wikipedia.org/wiki/Web_scraping) is the process of extracting data from html code on the internet. \n",
    "\n",
    "Resources on web scraping:\n",
    "* [Digital Ocean Tutorial](https://www.digitalocean.com/community/tutorials/how-to-scrape-web-pages-with-beautiful-soup-and-python-3) (requests, Beautiful Soup)\n",
    "* [DataCamp Tutorial](https://www.datacamp.com/community/tutorials/web-scraping-using-python) (urllib, Beautiful Soup)\n",
    "* [Hitchhiker's Guide to Python](https://docs.python-guide.org/scenarios/scrape/) (requests, lxml) \n",
    "\n",
    "**Important Note**: This is for demonstration purposes only, and only harvests content from individual pages. When building a scraper to harvest large amounts of data from multiple pages, be mindful of [legal](https://www.fastcompany.com/40456140/bots-are-scraping-your-public-data-for-cash-amid-murky-laws-and-ethics-linkedin-hiq) and [ethical](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01) issues in web scraping.  \n",
    "\n",
    "## Today's Exercise\n",
    "Say we want to learn more about where Google's offices are located. Helpfully, the provide a list of all of their campuses globally at [google.com/about/locations](https://www.google.com/about/locations). However, copying this list by hand to do data analysis on would be frustrating and time-consuming. Let's take a look at how web scraping can make this process easier. \n",
    "\n",
    "First, let's import all of the packages we'll need for today's exercise. There are a wide variety of packages that can be helpful, but today we'll be using *requests* and *Beautiful Soup* to pull the contents of these websites. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "64222d56-23be-497b-8b20-86f95d5d019d"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a6eb6643-27c0-4a69-8c13-8ce5b2dfb344"
    }
   },
   "source": [
    "First, we use the *requests* package to get the content of the google site we'd like to extract office location information from: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "c53abd25-3d4e-427b-a912-91e3e089886a"
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://www.google.com/about/locations/'\n",
    "site_source = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5b24ab79-0f0f-4003-9062-29352ff37f4c"
    }
   },
   "source": [
    "This is going to give us an enormous amount of content - everything we would get if we looked directly at the source code in the browser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a2a59238-c7f0-422d-bc72-4c54c971a037"
    }
   },
   "outputs": [],
   "source": [
    "print(site_source.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "36b2aa88-ab70-4524-be45-02e4459ae2d4"
    }
   },
   "source": [
    "#### We could parse this ourselves, but fortunately scraping packages make this much easier\n",
    "\n",
    "We'll use Beautiful Soup's built in functionality to extract info on the individual offices.\n",
    "\n",
    "First, we parse the full site content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "79d6aa7f-29dc-425f-9cc1-685a1ea9670b"
    }
   },
   "outputs": [],
   "source": [
    "site_content = soup(site_source.content, \"html.parser\")\n",
    "\n",
    "type(site_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we pick out the office elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "4fb05d6c-5e3b-422d-b85b-847a5a4d2a20"
    }
   },
   "outputs": [],
   "source": [
    "offices = site_content.select(\".office-info\")\n",
    "\n",
    "len(offices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_content.select(\".office-info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've isolated the office elements, let's extract the location name and address for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "9dbadf6d-adf6-4bb3-93d7-26acf89b51dd"
    }
   },
   "outputs": [],
   "source": [
    "countries = []\n",
    "for o in offices:\n",
    "    office = o.select(\".office-name\")[0].string.strip()\n",
    "    address = o.select(\".office-address\")[0].string.strip()\n",
    "    \n",
    "    address_list = re.split(r'\\n|\\,', address)\n",
    "    country = address_list[-1].strip()\n",
    "    if country not in countries:\n",
    "        countries.append(country)\n",
    "        print(country)\n",
    "        \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we look carefully at our extracted elements, we'll see we have some issues:\n",
    "1. All elements appear twice.\n",
    "2. The zip codes - which we're interested in - are part of broader strings. \n",
    "\n",
    "These are trivial to handle, we'll just need to pass over the data carefully to handle both. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "64eb4da2-163a-41e8-82ce-e1f21bde4cf4"
    }
   },
   "outputs": [],
   "source": [
    "us_offices = []\n",
    "for o in offices:\n",
    "    office = o.select(\".office-name\")[0].string.strip()\n",
    "    address = o.select(\".office-address\")[0].string.strip()\n",
    "\n",
    "    is_US = re.search(r'(United States)', address)\n",
    "\n",
    "    if is_US:\n",
    "        \n",
    "        print(office)\n",
    "        zip_code = re.search(r'(\\d{5})', address)\n",
    "        if zip_code:\n",
    "            print(zip_code.group())\n",
    "            if [office, zip_code.group()] not in us_offices:\n",
    "                us_offices.append([office, zip_code.group()])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've extracted a list of offices and zip codes, we can load them into a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "00ef91b7-75fb-4ce9-a87c-1b016152bec2"
    }
   },
   "outputs": [],
   "source": [
    "office_df = pd.DataFrame(us_offices, columns=['Office', 'Zip Code'])\n",
    "\n",
    "office_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: In What Countries Does Google Maintain Offices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "251758a0-1b93-4b20-98e3-6020d61940d6"
    }
   },
   "source": [
    "## Adding County Information\n",
    "\n",
    "The Department of Housing and Urban Development makes a *crosswalk* of zip codes to counties available [here](https://www.huduser.gov/portal/datasets/usps_crosswalk.html). We can load these into pandas and clean them up to find the county for each office. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "0f22f7c6-68bb-4c90-839e-23f83c0820aa"
    }
   },
   "outputs": [],
   "source": [
    "zip_df = pd.read_csv('https://grantmlong.com/data/ZIP_COUNTY_122016.csv')\n",
    "\n",
    "zip_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "25d79f61-f8e1-4a72-9777-2b4cab6b5fbc"
    }
   },
   "source": [
    "#### A good rule of thumb: if two numbers cannot be added together to produce a logical result, they should be stored as strings. '\n",
    "\n",
    "We can recast the zip and county ids as strings - with leading zeros - to make this dataframe easier to handle. To do this we can use the [.astype()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.astype.html) and [.zfill()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.zfill.html) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e2a1fd05-ed4d-485a-924c-f4a40facc341"
    }
   },
   "outputs": [],
   "source": [
    "zip_df['Zip Code'] = zip_df['ZIP'].astype(str).str.zfill(5) \n",
    "zip_df['County Number'] = zip_df['COUNTY'].astype(str).str.zfill(5) \n",
    "\n",
    "zip_df.sort_values(by='COUNTY').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5fbd2350-d7ef-4054-aec1-8dc557401f08"
    }
   },
   "source": [
    "Of course we don't need all of these columns, but we do need to attach the ***County Number*** column to our Google office data in order to learn more about the data. The [.merge()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html) method allows us to do this easily in one line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e50bd618-e6a6-4e00-9fad-c84746b7105a"
    }
   },
   "outputs": [],
   "source": [
    "office_df = office_df.merge(zip_df[['Zip Code', 'County Number']], \n",
    "                            how='left')\n",
    "\n",
    "office_df.sort_values(by='Office')\n",
    "office_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "0b8cebc2-3397-47d0-9551-322b71f2ebc0"
    }
   },
   "outputs": [],
   "source": [
    "office_df.sort_values(by='County Number')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "af1ec4fe-302e-4a76-850a-5698b51f4904"
    }
   },
   "source": [
    "### Merge Office Data with Census Data\n",
    "First, we'll need to load the data extract we produced in the first data dive. We'll also need to make sure that the *County Number* - the variable we need to join our data on - is appropriately formatted as a string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "1e01f411-a7f5-4639-8b45-cf5e417a573b"
    }
   },
   "outputs": [],
   "source": [
    "census_df = pd.read_csv('https://grantmlong.com/data/census_counties_backup.csv')\n",
    "census_df['County Number'] = census_df['County Number'].astype(str).str.zfill(5) \n",
    "\n",
    "census_df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use the [.merge()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html) method to attach the two data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "ae84d2e9-6299-4b3d-b649-9fd22fbf4c95"
    }
   },
   "outputs": [],
   "source": [
    "full_df = census_df.merge(office_df, how='left')\n",
    "full_df.loc[full_df['Office'].notnull(), ].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### With our full data set, we can now begin to look at some interesting numbers, like the median income in counties where google has an office, and where they don't. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "041ac846-3824-4740-b951-0a625ff1c201"
    }
   },
   "outputs": [],
   "source": [
    "print(full_df['Median Rent'].mean())\n",
    "print(full_df.loc[full_df['Office'].notnull(), 'Median Rent'].mean())\n",
    "print(full_df.loc[full_df['Office'].isnull(), 'Median Rent'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "bc1cd633-9c04-4580-97e7-398c7af759bb"
    }
   },
   "outputs": [],
   "source": [
    "(full_df.loc[full_df['Office'].notnull(),]\n",
    " .sort_values(by='Median Rent', \n",
    "              ascending=False)\n",
    " .head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: What Other Interesting Findings Can We Identify?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
