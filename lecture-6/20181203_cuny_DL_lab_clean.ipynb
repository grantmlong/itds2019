{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lT63quvgri0L"
   },
   "source": [
    "* The empty ipynb for you to start from in the repo: https://github.com/grantmlong/itds2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FTApi-7voeCo"
   },
   "source": [
    "# Setting the stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fP6QeXnVeWzi",
    "outputId": "4b1a109d-f35d-4a33-b758-bff725a5dcf3"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version # 3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "id": "fHza7chYecM4",
    "outputId": "0ec0e972-728b-447b-a5ba-91a9bbe9dada"
   },
   "outputs": [],
   "source": [
    "# if running on colab, pytorch is already installed.\n",
    "# if running locally, conda or pip install this in your conda environment:\n",
    "# conda install pytorch torchvision -c pytorch\n",
    "# OR\n",
    "# pip3 install torch torchvision\n",
    "\n",
    "# I'll be assuming python >=3.6 and torch 1.0.1 which are the colab defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ropMkTLTiWos",
    "outputId": "9cba56ae-1b3c-4bd1-f85f-f9c7e83fa9ed"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "print(torch.__version__) # 1.0.1\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zgfEx8YN0BDF"
   },
   "source": [
    "# Torch and autograd basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch is a package that defines vectors, matrices, or in general \"tensors\". If you know numpy, you will not be surprised by any of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UmvFvWHJ0GQ1"
   },
   "outputs": [],
   "source": [
    "a = torch.ones(3,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mur2vsHp0GmX"
   },
   "outputs": [],
   "source": [
    "b = torch.arange(9).float().view(3,3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_hQiSHvW0G2v"
   },
   "outputs": [],
   "source": [
    "(a+b)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.zero_() # operations with an underscore modify the Tensor in place.\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can slice and dice tensors and they have roughly all tensor operations you expect equivalently to numpy, but with a bit more low level control. If you need more intro: https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py\n",
    "\n",
    "So what's the big deal about pytorch?\n",
    "\n",
    "**autograd = automatic differentiation**.\n",
    "\n",
    "Every `torch.Tensor`, let's say `x`, has an important flag `requires_grad`. If this flag is set to True, pytorch will keep track of the graph of operations that happen with this tensor.\n",
    "When we finally arrive at some output (a scalar variable based on a sequence of operations on `x`), we can call `.backward()` on this output, to compute the gradient `d(output) / dx`. This gradient will end up in `x.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2,2, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=(x**2 + x)\n",
    "z = y.sum()\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know from high school math that the derivative `dz / dx[i,j]` = 2*x +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2*x+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the intermediate variable y? Does it require a gradient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However the gradient of y is not exposed, since it is an intermediary variable, the result of an operation on leaf variables. Leaf variables are inputs to the operations: the data X or the `Parameter`s of a neural network.\n",
    "\n",
    "More about autograd in the tutorial https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py and the docs https://pytorch.org/docs/stable/autograd.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we talked about how derivatives and backpropagation (based on the chain rule of differentiation) play a central role in deep learning. You can now start to see how this autograd will be massively powerful to define neural networks with weights `W` and optimize them based on the gradients in `W.grad`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression on a toy problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this for a simple linear mapping `y = W x `, where we want to optimize W:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(23801)\n",
    "x1 = torch.Tensor([1, 2, 3, -3, -2])\n",
    "y = torch.Tensor ([3, 6, 9, -9, -6]).view(5,1)\n",
    "x2 = torch.randn(5)\n",
    "x = torch.stack([x1, x2], dim=1) # 5 x 2 input. 5 datapoints, 2 dimensions.\n",
    "# clearly the true relationship is y  = 3*x1 + 0 * x2.  So we can see that the optimal W = [3,0]\n",
    "W = torch.randn(1,2, requires_grad=True)\n",
    "# we start W at random initialization, the gradient will point us in the right direction.\n",
    "print('x:\\n', x)\n",
    "print('y:\\n', y)\n",
    "print('W at random initialization: ', W)\n",
    "# Now let's add some noise to the second column of x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in notation of previous lecture 5, \n",
    "* the features are still $X_1, X_2$ concatenated as columns (rows = datapoints)\n",
    "* $W = [\\beta_1, \\beta_2]$\n",
    "* there is no $\\beta_0$ or \"intercept term\" or in deep learning speak, \"bias\" term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = x @ W.t() # matrix multiply; (N x 2) * (2 x 1) -> N x 1\n",
    "print('ypred:\\n', ypred)\n",
    "loss = ((ypred-y)**2).mean() # mean squared error = MSE\n",
    "print('mse loss: ', loss.item())\n",
    "loss.backward()\n",
    "print('W grad:\\n', W.grad)\n",
    "# let's move W in that direction\n",
    "W.data -= 0.1 * W.grad\n",
    "# Now we will reset the gradient to zero.\n",
    "# Do an in-place operation tensor operation `.zero_()` on the gradient W.grad\n",
    "W.grad.zero_()\n",
    "print('W:\\n', W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can re-execute this cell above a couple of times and see how W goes close towards the optimal value of `[3,0]` (it may be a bit off because of the added noise).\n",
    "\n",
    "torch defines `Module`s which do two things: (a) they contain the learnable weight, and (b) define how they operate on an input tensor to give an output.\n",
    "In this case this would be a `Linear` layer, reducing 2D datapoints `x` to 1D output `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w00A5MF07pIw"
   },
   "source": [
    "Ok doing this manually gives you insight what happens down to the gradienst. But usually we do not do these things manually, it would become very cumbersome if the net becomes more complex than the simple linear layer. pytorch gives us primitives to do the same: `net.zero_grad()` to clear the gradients, and for optimization you can do `optimizer.step()` to do a step of SGD.\n",
    "Again we will do 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(2,1, bias=False)\n",
    "linear.weight.data.copy_(W) # we re-initialize the linear layer with the W we just found\n",
    "ypred = linear(x)\n",
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with a `torch.nn.Linear` layer you can do the same thing we just did:\n",
    "* forward propagate (=compute prediction), \n",
    "* compute MSE loss, and \n",
    "* backprop, then \n",
    "* update the linear layer's weight according to the gradient.\n",
    "\n",
    "Let's also change two things\n",
    "* do this in a loop, instead of re-executing the cell many times which was silly\n",
    "* use an \"optimizer\" which takes care of adding the gradient to the weight in `linear.weight`\n",
    "* (homework: break this down to the way we did it above, and check it matches.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(23801)\n",
    "linear = nn.Linear(2,1, bias=False)\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.1)\n",
    "# x, y defined above. In a real problem we would typically get different x, y \"minibatches\"\n",
    "# of samples from a dataloader.\n",
    "for i in range(10): # 10 optimization steps (gradient descent steps)\n",
    "    ypred = linear(x)\n",
    "    loss = ((ypred-y)**2).mean() # mean squared error = MSE\n",
    "    print('mse loss:', loss.item())\n",
    "    linear.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revisiting linear regression from lab 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us:\n",
    "* redo the linear regression example of previous lecture 5 (I provide the code to get the X, Y from the dataframe as torch Tensors)\n",
    "* For adding a $\\beta_0$ intercept term, equivalent to `fit_intercept=True`, in torch you can set `nn.Linear(..., bias=True)`.\n",
    "* At every time step, we take only a minibatch = random subset of the data. upper case X,Y = full, lower case x,y = minibatch\n",
    "* Note that the loop for optimizing the Linear layer will be the equivalent of `lm.fit(X, se_df.rent)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_df = pd.read_csv('https://grantmlong.com/data/streeteasy_rents_june2016.csv')\n",
    "# Use sensible subset of predictors to fit linear regression model\n",
    "# dependent_vars = ['bedrooms', 'bathrooms', 'min_to_subway', 'floor', \n",
    "#                   'building_age_yrs', 'no_fee', 'has_roofdeck', \n",
    "#                   'has_washer_dryer', 'has_doorman', 'has_elevator', \n",
    "#                   'has_dishwasher', 'has_patio', 'has_gym']\n",
    "dependent_vars = ['size_sqft']\n",
    "\n",
    "\n",
    "X = se_df[dependent_vars]\n",
    "Y = se_df.rent\n",
    "X = torch.from_numpy(X.values).float() # df.values returns np array, torch.from_numpy maps to torch Tensor\n",
    "Y = torch.from_numpy(Y.values).float()\n",
    "# normalize the features and regression targets (note need to rescale afterwards again)\n",
    "# Xmean, Xstd = X.mean(0, keepdim=True), X.std(0, keepdim=True)\n",
    "# X = (X - Xmean) / Xstd\n",
    "Ymean, Ystd = Y.mean(), Y.std()\n",
    "Y = (Y - Ymean) / Ystd\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a similar loop as above to optimize the linear layer:\n",
    "LEARNING_RATE  = 1.0\n",
    "MINIBATCH_SIZE = 5000\n",
    "OPTIMIZATION_STEPS = 100\n",
    "torch.manual_seed(23801)\n",
    "linear = nn.Linear(len(dependent_vars), 1, bias=True)\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=LEARNING_RATE)\n",
    "# X, y defined above. You could joi get different x, y \"minibatches\"\n",
    "# of samples from a dataloader.\n",
    "for i in range(OPTIMIZATION_STEPS): # 100 optimization steps (gradient descent steps)\n",
    "#     subset_ix = torch.randperm(X.shape[0])[:MINIBATCH_SIZE]\n",
    "#     x, y = X[subset_ix, :], Y[subset_ix]\n",
    "    x, y = X, Y\n",
    "    # TODO optimize the linear layer with the mse loss.\n",
    "    ypred = linear(x)\n",
    "    loss = ((ypred-y)**2).mean() # mean squared error = MSE\n",
    "    linear.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(linear.parameters(), 1, norm_type=2)\n",
    "    optimizer.step()\n",
    "    if i % 100 == 0:\n",
    "        print('mse loss:', loss.item())\n",
    "#         print('w', linear.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear.weight.data.fill_(5.384)\n",
    "linear.bias.data.fill_(-416.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = plt.scatter(se_df['rent'], (linear(X) * Ystd + Ymean ).detach().numpy())\n",
    "_ = plt.scatter(se_df['rent'], linear(X).detach().numpy())\n",
    "_ = plt.xlabel(\"Rents: $Y_i$\")\n",
    "_ = plt.ylabel(\"Predicted rents: $\\hat{Y}_i$\")\n",
    "_ = plt.title(\"Rents vs Predicted Rents: $Y_i$ vs $\\hat{Y}_i$\")\n",
    "_ = plt.plot([0, 20000], [0, 20000], linewidth=4, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xJwW0C5Yw1Rg"
   },
   "source": [
    "Important questions to revisit after you learned about overfitting:\n",
    "* How many parameters (weights) does our linear regression model have? How many  datapoints did we train on?\n",
    "* Old-skool machine learning rule of thumb is: you can optimize about as many parameters (weights) as you have datapoints before you can memorize the dataset (thus overfit heavily). Are we close to the limit?\n",
    "* Does the model overfit? Measure this by splitting off a heldout set after loading the data on which you do not train, and measure performance on the heldout set.\n",
    "* In deep neural networks you can easily have way more parameters than datapoints. Is overfitting an issue for neural networks in general?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjVmBKaavPJP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jdh_P4DKoNup"
   },
   "source": [
    "# Now the real stuff: MNIST classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST is a dataset of 50k handwritten digits (0-9) which is very commonly used in the deep learning community.\n",
    "It is small enough to work with locally (on your laptop) and without much hassle, and complex enough to do something interesting with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LaHzzHci0m5i"
   },
   "source": [
    "The loss we will optimize is  $$\\mathcal{L(\\theta)} = \\sum_{x,y_{t} \\in D} \\ell(x,y_t; \\theta)$$\n",
    "with $$\\ell(x,y_t; \\theta) = \\text{NLL}(\\hat{y}(x; \\theta) || y_t)$$ the negative log likelihood loss. https://pytorch.org/docs/stable/nn.html#torch.nn.NLLLoss\n",
    "\n",
    "Now we'll set up a small neural network to learn a good $\\hat{y}(x; \\theta)$. \n",
    "The variable $\\theta$ simply means all the neural network weights together, which we will update on each step. So per sample, the neural network output $\\hat{y}(x; \\theta)$ is the model's prediction of the probability of each class.  This is being compared to the true label $y_t$.\n",
    "The loss value is a metric of how far off the predictions are.\n",
    "We want to minimize this loss with SGD (stochastic gradient descent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow the typical training procedure for a neural network which is as follows:\n",
    "\n",
    "* Define the neural network that has some learnable parameters (or weights) $\\theta$.\n",
    "* Iterate over a dataset of inputs\n",
    "* Process input through the network\n",
    "* Compute the loss (how far is the output from being correct)\n",
    "* Propagate gradients back into the network’s parameters\n",
    "* Update the weights of the network, typically using SGD, i.e. the simple update rule: weight = weight - learning_rate * gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "PH8zthzBhjKS",
    "outputId": "d959308e-faa5-4912-a44e-d5be1001b65b"
   },
   "outputs": [],
   "source": [
    "# let's download the MNIST data, if you do this locally and you downloaded before,\n",
    "# you can change data paths to point to your existing files\n",
    "train_dataset = dsets.MNIST(root='./MNISTdata',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./MNISTdata',\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CNjVlTt87ou1"
   },
   "source": [
    "Dataset and DataLoader are abstractions to help us iterate over the data in random order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the digits and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix=129\n",
    "x,y = train_dataset[ix]\n",
    "plt.imshow(x.squeeze().numpy())\n",
    "plt.colorbar()\n",
    "print('label: y={}'.format(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the dataloaders and train simple neural network like before.\n",
    "You'll recognize that the core is exactly the same: we do a forward pass, compute a loss, backpropagate the loss to compute the gradients, then let the optimizer update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tONXsS3FtvFK"
   },
   "outputs": [],
   "source": [
    "# The neural network hyperparameters.\n",
    "input_size    = 784   # The MNIST image size = 28 x 28 = 784\n",
    "hidden_size   = 100   # The number of nodes at the hidden layer\n",
    "num_classes   = 10    # The number of output classes. In this case, from 0 to 9\n",
    "num_epochs    = 5     # The number of times entire dataset is trained\n",
    "batch_size    = 100   # The number of samples per minibatch\n",
    "learning_rate = 1.0   # SGD step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "colab_type": "code",
    "id": "UZoB9O6ce0js",
    "outputId": "b20332d2-7abd-42f5-f788-452591dccdde"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu') # if on gpu-enabled machine, change to torch.device('cuda')\n",
    "\n",
    "#  define simple neural network with 1 hidden layer.\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()                    # Inherited from the parent class nn.Module\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # 1st Full-Connected Layer: 784 (input data) -> 500 (hidden node)\n",
    "        self.relu = nn.ReLU()                          # Non-Linear ReLU Layer: max(0,x)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes) # 2nd Full-Connected Layer: 500 (hidden node) -> 10 (output class)\n",
    "    \n",
    "    def forward(self, x):                              # Forward pass: stacking each layer together\n",
    "        x = x.view(x.size(0), -1) # flatten (bs x 1 x 28 x 28) -> (bs x 784)\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "def test(net, dl):\n",
    "    right, tot = 0, 0\n",
    "    net.eval() # Set dropout and possibly other modules in eval mode.\n",
    "    for x,y in dl:\n",
    "        ypred = net(x).argmax(dim=1) # select index of maximal score\n",
    "        right += (ypred == y).sum().item()\n",
    "        tot   += x.size(0)\n",
    "    return 1.* right / tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the net based on this class definition\n",
    "net = Net(input_size, hidden_size, num_classes).to(device)\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "print('Before training: {:.1f}% test accuracy'.format(100*test(net, test_loader)))\n",
    "for epoch in range(num_epochs):\n",
    "    scheduler.step()\n",
    "    for i, (x,y_target) in enumerate(train_loader):\n",
    "        x, y_target = x.to(device), y_target.to(device)\n",
    "        y_probs = F.log_softmax(net(x), dim=1)\n",
    "        output = F.nll_loss(y_probs, y_target)\n",
    "        output.backward()\n",
    "        optimizer.step()\n",
    "        net.zero_grad()\n",
    "    print('After epoch {}: {:.1f}% test accuracy'.format(epoch, 100*test(net, test_loader)))\n",
    "print('End of training: {:.1f}% train accuracy'.format(100*test(net, train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the prediction on some samples.\n",
    "ix=1234\n",
    "x,y = test_dataset[ix]\n",
    "plt.imshow(x.squeeze().numpy())\n",
    "print('Ground truth label: y={}'.format(y))\n",
    "y_probs = F.softmax(net(x), dim=1)\n",
    "print ('Model probabilities: ') \n",
    "print(' / '.join(['{}: {:.3f}'.format(k,v) \n",
    "                  for k,v in zip(range(10), y_probs.squeeze().tolist()) ] ))\n",
    "print('Model prediction: ', y_probs.argmax(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila and that's how it's done. You can play around to figure out the answer to some more questions:\n",
    "* What do we get back from `net.parameters()`: which trainable weights and biases does the network have now? \n",
    "* How many total parameters?\n",
    "* Can you add another hidden layer to the class definition of `Net`? How does the performance change (watch both test & train)?\n",
    "* Can you add a dropout layer after the first and second hidden layer? How does performance change now? See https://pytorch.org/docs/master/nn.html#torch.nn.Dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some more fundamental questions for you to understand what's going on in the optimization loop:\n",
    "* what does softmax do? (compute softmax of a random vector, then sum the output, it will sum to 1)\n",
    "* what is its purpose? (read the docs)\n",
    "* what does nll_loss compute? can you manually compute it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we used a simple flat neural network which looks at the image as a flat vector, without awareness of the 2D structure or which pixels neighbor each other.\n",
    "A convolutional neural network is an architecture that takes the 2D structure of the image into account by sliding a kernel over all the different locations in the image. This kind of neural network has been very succesful in image recognition [1] and speech recognition [2,3].\n",
    "Pytorch and other deep learning toolboxes are designed to deal with this kind of data and with convolutional neural networks just as easily as with flat data. Try swapping out the network above for a convolutional neural network, see for example the pytorch tutorial [4].\n",
    "\n",
    "[1] https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks\n",
    "[2] http://www.cs.toronto.edu/~asamir/papers/icassp13_cnn.pdf\n",
    "[3] https://arxiv.org/abs/1509.08967\n",
    "[4] https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py\n",
    "[5] https://colab.research.google.com/drive/1jxUPzMsAkBboHMQtGyfv5M5c7hU8Ss2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have fun exploring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "48JIxr6HoXH6"
   },
   "source": [
    "# Finishing notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZUsyDpADjBbI"
   },
   "source": [
    "Inspiration for this lab and the lecture:\n",
    "\n",
    "*  An old lab I made in lua torch https://github.com/tomsercu/torchtutorial\n",
    "* This pytorch intro notebook https://colab.research.google.com/drive/1jxUPzMsAkBboHMQtGyfv5M5c7hU8Ss2c\n",
    "* The official pytorch tutorial https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "* Yann LeCuns deep learning course in 2015 https://cilvr.nyu.edu/doku.php?id=deeplearning2015:schedule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HJog1HK1nRf7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "jdh_P4DKoNup",
    "48JIxr6HoXH6"
   ],
   "name": "20181203_cuny_DL_lab",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
