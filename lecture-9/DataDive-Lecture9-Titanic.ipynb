{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Dive Week 9: Decision Trees\n",
    "\n",
    "This week we take a look at *decision trees*, our second type of classification model that brings deeper into the machine learning territory. We'll be using `scikit-learn` in today's exercise. \n",
    "\n",
    "\n",
    "***\n",
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/d/db/Titanic-Cobh-Harbour-1912.JPG/330px-Titanic-Cobh-Harbour-1912.JPG)\n",
    "    \n",
    "This week we'll be illustrating how decision trees work using the Titanic survivor dataset available on [Kaggle](https://www.kaggle.com/c/titanic/data). We'll look at a create variety of variables to help us learn predict whether a given passenger on the Titanic was able to survive. There is a ton out on the web (including [here](https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/)) about this dataset, as it's a popular among those just coming up to speed on machine learning classification models. Play around and use what you learn in class to join [the Kaggle competition](https://www.kaggle.com/c/titanic)!. \n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary\n",
    "\n",
    "|Variable|Definition|Key|\n",
    "| --- | --- |:---|\n",
    "| survival | Survival | 0 = No, 1 = Yes |\n",
    "| pclass\t| Ticket class\t| 1 = 1st, 2 = 2nd, 3 = 3rd |\n",
    "| sex\t| Sex | | \t\n",
    "| Age | Age in years | | \t\n",
    "| sibsp\t| # of siblings / spouses aboard the Titanic | \t| \n",
    "| parch\t| # of parents / children aboard the Titanic | | \n",
    "| ticket | Ticket number | | \n",
    "| fare\t| Passenger fare | \t| \n",
    "| cabin\t| Cabin number| \t| \n",
    "| embarked | Port of Embarkation\t| C = Cherbourg (France), Q = Queenstown (Ireland), S = Southampton (England) | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pydot\n",
    "\n",
    "# Used for visualizing trees, but not strictly necessary\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and summarize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://grantmlong.com/data/titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Survived.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summarize survival by age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df.Survived==0), 'Age'].hist(bins=20, alpha=.6, color='red', figsize=[15, 5])\n",
    "df.loc[(df.Survived==1), 'Age'].hist(bins=20, alpha=.6, color='blue')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summarize survival by sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Sex', 'Survived']].groupby('Sex').agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find and Count Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TODO: Summarize by Pclass, point of embarkment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Pclass', 'Survived']].groupby(['Pclass']).agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Embarked', 'Survived']].groupby(['Embarked']).agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Embarked', 'Pclass', 'Survived']].groupby(['Embarked', 'Pclass']).agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Feature Engineering\n",
    "\n",
    "Sadly `sci-kit learn` will only let use numeric or boolean variables to train our decision tree, so let's transform some of our variables to address that. \n",
    "* Create booleans for each of the Embarkment points.\n",
    "* Create a boolean for is_male. \n",
    "* Create a boolean for whether someone has a cabin. \n",
    "* **TODO, time permitting:** create identifiers for passengers in A, B, C, and D cabins\n",
    "\n",
    "Moreover, some of our ages are missing, so let's enter the missing values as 100 for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embarkment booleans\n",
    "for k in df.Embarked.unique():\n",
    "    if type(k)==str:\n",
    "        df['emb_' + k] = (df.Embarked==k)*1\n",
    "\n",
    "# Sex boolean\n",
    "df['is_male'] = (df.Sex=='male')*1\n",
    "\n",
    "# Has cabin boolean\n",
    "df.loc[:, 'has_cabin'] = 0\n",
    "df.loc[df.Cabin.isna(), 'has_cabin'] = 1\n",
    "\n",
    "# Age fill\n",
    "df.loc[df.Age.isna(), 'Age'] = 100\n",
    "\n",
    "print(list(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's assign a list of our clean and model ready features to a list so we can call them easily while training our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', \n",
    "            'emb_S', 'emb_C', 'emb_Q', 'is_male', 'has_cabin']\n",
    "\n",
    "valid = df[features].notna().all(axis=1)\n",
    "print(len(df), sum(valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Decision Tree\n",
    "\n",
    "Now that we have variables in good shape, we can start modeling. Let's train a simple tree and see how it performs.   \n",
    "Note: for the documentation on `DecisionTreeClassifier`, see [here](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree=DecisionTreeClassifier(\n",
    "    criterion='entropy', \n",
    "    random_state=20181105, \n",
    "    #max_depth=5,\n",
    "    min_samples_split=2, \n",
    "    #min_samples_leaf=1, \n",
    "    #max_features=None, \n",
    "    #max_leaf_nodes=None, \n",
    ")\n",
    "\n",
    "dtree.fit(df[features], df['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the tree. *Note: there's a strong chance this will not work if you do not have `graphviz` installed.* \n",
    "For more on visualizing decision trees see [here](https://chrisalbon.com/machine_learning/trees_and_forests/visualize_a_decision_tree/), and for more on installing graphviz see [here](https://graphviz.gitlab.io). To install `graphviz` on my Macbook Air, I used `brew install graphviz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "export_graphviz(dtree, \n",
    "                out_file=dot_data,  \n",
    "                filled=True, \n",
    "                rounded=True,\n",
    "                feature_names=features,\n",
    "                special_characters=True\n",
    "               )\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate metrics from in-sample performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_survival = dtree.predict(df[features])\n",
    "\n",
    "print(confusion_matrix(df.Survived, pred_survival), '\\n')\n",
    "print('Accuracy:   %0.3f' % accuracy_score(df.Survived, pred_survival))\n",
    "print('Precision:  %0.3f' % precision_score(df.Survived, pred_survival))\n",
    "print('Recall:     %0.3f' % recall_score(df.Survived, pred_survival))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait, are nonlinear models actually doing better here? \n",
    "* Let's run a logistic regression to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=20181105, solver='lbfgs')\n",
    "logreg.fit(df[features], df['Survived'])\n",
    "pred_survival = logreg.predict(df[features])\n",
    "\n",
    "print(confusion_matrix(df.Survived, pred_survival), '\\n')\n",
    "print('Accuracy:   %0.3f' % accuracy_score(df.Survived, pred_survival))\n",
    "print('Precision:  %0.3f' % precision_score(df.Survived, pred_survival))\n",
    "print('Recall:     %0.3f' % recall_score(df.Survived, pred_survival))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Hyperparameters with Cross Validation\n",
    "* First, we use the `KFold` function from `sci-kit learn` to generate five folds for cross validation. We can show the balance of the survivor rate among the different folds to get a better idea of what's going on.\n",
    "* Next, we train a different decision tree model against each of the folds and track our performance.\n",
    "* Finally, we track average cv metrics for different values of our hyperparameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits=5, random_state=20181105)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of observations and survivor rate  for \n",
    "for train_indices, test_indices in k_fold.split(df[features]):\n",
    "     print('Train: n=%i, s_rate=%0.2f | test: n=%i, s_rate=%0.2f ' % \n",
    "           (df.loc[train_indices, 'Survived'].count(), \n",
    "            df.loc[train_indices, 'Survived'].mean(), \n",
    "            df.loc[test_indices, 'Survived'].count(),\n",
    "            df.loc[test_indices, 'Survived'].mean(),\n",
    "           )\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a function to fit our model and return relevant metrics makes it easy to track cross validation performance over different values of our parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_results(classifier):\n",
    "    results = []\n",
    "    for train, test in k_fold.split(df[features]):\n",
    "        classifier.fit(df.loc[train, features], df.loc[train, 'Survived'])\n",
    "        y_predicted = classifier.predict(df.loc[test, features])\n",
    "        accuracy = accuracy_score(df.loc[test, 'Survived'], y_predicted)\n",
    "        results.append(accuracy)\n",
    "    \n",
    "    return np.mean(results), np.std(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's track mean and variance of accuracy for different values of the minimum samples per split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_values = range(10,200, 10)\n",
    "all_mu = []\n",
    "all_sigma = []\n",
    "\n",
    "for m in hp_values:\n",
    "\n",
    "    dtree=DecisionTreeClassifier(\n",
    "        criterion='entropy', \n",
    "        random_state=20180408, \n",
    "        min_samples_split=m, \n",
    "        #max_depth=m,\n",
    "        #min_samples_leaf=m, \n",
    "        #max_features=m, \n",
    "        #max_leaf_nodes=m, \n",
    "    )\n",
    "\n",
    "    mu, sigma = get_cv_results(dtree)\n",
    "    all_mu.append(mu)\n",
    "    all_sigma.append(sigma)\n",
    "    \n",
    "    print(m, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(hp_values, all_mu)\n",
    "plt.ylabel('Cross Validation Accuracy')\n",
    "plt.xlabel('Minimum Samples Per Leaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(hp_values, all_sigma)\n",
    "plt.ylabel('Cross Validation Std Dev.')\n",
    "plt.xlabel('Minimum Samples Per Leaf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretty cool, right? We can take a quick look again at how these results compare to logistic regression.\n",
    "\n",
    "* What do you make of these results?\n",
    "* Is this a better model? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=20181105, solver='lbfgs')\n",
    "get_cv_results(logreg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Our Model and Applying It to Our Test Set\n",
    "#### From this, it seems like `min_samples_split=100` might provide our best fit. We can train our best model using that value.  \n",
    "\n",
    "We can then read in our holdout test set from the Kaggle competition to enter our predictions. We'll first double check and see if our model makes sense by taking a closer look at our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree=DecisionTreeClassifier(\n",
    "        criterion='entropy', \n",
    "        random_state=20181105, \n",
    "        min_samples_split=100, \n",
    "    )\n",
    "\n",
    "# Here we train our final model against all of our validation data. \n",
    "dtree.fit(df.loc[:, features], df.loc[:, 'Survived'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read in our test data and apply the same transformations as our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('https://grantmlong.com/data/titanic_test.csv')\n",
    "\n",
    "# Embarkment booleans\n",
    "for k in test_df.Embarked.unique():\n",
    "    if type(k)==str:\n",
    "        test_df['emb_' + k] = (test_df.Embarked==k)*1\n",
    "\n",
    "# Sex boolean\n",
    "test_df['is_male'] = (test_df.Sex=='male')*1\n",
    "\n",
    "# Has cabin boolean\n",
    "test_df.loc[:, 'has_cabin'] = 0\n",
    "test_df.loc[test_df.Cabin.isna(), 'has_cabin'] = 1\n",
    "\n",
    "# Age fill\n",
    "test_df.loc[test_df.Age.isna(), 'Age'] = 100\n",
    "\n",
    "# Fare fill\n",
    "test_df.loc[test_df.Fare.isna(), 'Fare'] = test_df.loc[test_df.Fare.notna(), 'Fare'].median()\n",
    "\n",
    "print(list(test_df))\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rank the most likely to survive according to our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the probability of \n",
    "test_probabilities = dtree.predict_proba(test_df[features])[:,1]\n",
    "test_df['survival_likelihood'] = test_probabilities\n",
    "\n",
    "readable_features =  ['Name', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', \n",
    "                      'Ticket', 'Fare', 'Cabin', 'Embarked', 'survival_likelihood']\n",
    "\n",
    "# Find the rankings based on the probabilities\n",
    "probability_rankings = np.argsort(test_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Most Likely to Survive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.loc[probability_rankings[-20:], readable_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Most Likely to Die:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.loc[probability_rankings[:20], readable_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
