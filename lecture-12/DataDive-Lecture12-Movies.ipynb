{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Dive 12: Clustering and Topic Modeling \n",
    "\n",
    "#### This week we'll use movie data from [the movie database (tMDB)](https://www.themoviedb.org/?language=en-US) available on [Kaggle](https://www.kaggle.com/tmdb/). \n",
    "\n",
    "> The Movie Database (TMDb) is a community built movie and TV database. Every piece of data has been added by our amazing community dating back to 2008. TMDb's strong international focus and breadth of data is largely unmatched and something we're incredibly proud of. Put simply, we live and breathe community and that's precisely what makes us different.\n",
    "\n",
    "<img src=\"https://cdn-image.travelandleisure.com/sites/default/files/styles/1600x1000/public/hollywood-dog-trvl1215.jpg?itok=6mQSRlLr\" width=600>\n",
    "\n",
    "*** \n",
    "#### Tech Specs\n",
    "Among some familiar tools from recents data dives, we'll be using [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) and [`KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) from `scikit-learn`. As we discussed in class, we'll also be using `altair`, `gensim`, and `pyLDAvis`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instruct Colab (or your personal machine) to install non-standard packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imUsingColab = True\n",
    "\n",
    "if imUsingColab:\n",
    "    !pip install gensim\n",
    "    !pip install pyLDAvis\n",
    "    !pip install vega\n",
    "    !pip install altair\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import non-standard packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attempt to load `altair`. Will not work without `altair` and `vega` pre-installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import altair as alt\n",
    "    if imUsingColab:\n",
    "        alt.renderers.enable('colab')\n",
    "    else:\n",
    "        alt.renderers.enable('notebook')    \n",
    "    imUsingAltair = True\n",
    "    print('Altair successfully loaded.')\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    imUsingAltair = False\n",
    "    print('Altair loading failed. Will default to matplotlib.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "random_state = 20181126"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data from web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://grantmlong.com/data/tmdb_5000_movies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop `na` values, filter for recent movies produced in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "cluster_vars = ['budget', 'popularity', 'revenue']\n",
    "df = df.dropna(subset=cluster_vars+['release_date'])\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['release_year'] = df.release_date.str.slice(0,4).astype(int)\n",
    "samp_df = df.loc[(df.release_year>=1900) &\n",
    "                 (df.original_language=='en')\n",
    "                ].reset_index()\n",
    "\n",
    "titles = {samp_df.title.loc[i] : i for i in samp_df.index.values}\n",
    "\n",
    "print(samp_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(samp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Vector of Normalized Data for Clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(samp_df[cluster_vars].astype(float))\n",
    "X = pd.DataFrame(scaler.transform(samp_df[cluster_vars].astype(float)), columns=cluster_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster with `KMeans`\n",
    "\n",
    "To start off, we'll create 5 clusters. Obviously we'll want to play with this to see what kinds of results we get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=8, \n",
    "                random_state=random_state)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "samp_df['cluster'] = (y_kmeans+1).astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results\n",
    "\n",
    "I've included code for `altair` hear for those interested. It's a lot nicer to use in this case because we'll be able to add tooltips to see which movies land where. If that's not working for you, you can always use `matplotlib` as a fall back.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_var = 'budget'\n",
    "y_var = 'revenue'\n",
    "\n",
    "if imUsingAltair:\n",
    "\n",
    "    chart = alt.Chart(\n",
    "        samp_df, \n",
    "        width=650,\n",
    "        height=400\n",
    "    ).mark_circle(\n",
    "        size=80\n",
    "    ).encode(\n",
    "        x=x_var,\n",
    "        y=y_var,\n",
    "        color='cluster',\n",
    "        tooltip=['title', 'revenue', 'cluster', 'release_date']    \n",
    "    ).interactive()\n",
    "\n",
    "else:\n",
    "    chart = plt.figure(figsize=[14,7])\n",
    "    chart = plt.scatter(samp_df[x_var], samp_df[y_var], c=samp_df.cluster.astype(float).values)\n",
    "    chart = plt.xlim([0, 300000000])\n",
    "    chart = plt.ylim([0, 300])\n",
    "    chart = plt.xlabel(x_var)\n",
    "    chart = plt.ylabel(y_var)\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling the Movies\n",
    "\n",
    "Our movie data also contains brief overviews of movie contents which we can use in topic modeling! Here, we'll transform the raw text into tokens for use in the bag-of-words style analysis we need to do topic modeling. \n",
    "\n",
    "*** \n",
    "\n",
    "First, let's take a look at a few overviews. Ideally, for the richest analysis, our documents would be a bit longer, but these should nonetheless give us some interesting results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.random.choice(df.index.values, 3):\n",
    "    print()\n",
    "    print(df.overview.loc[i], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's both vectorize and tokenize our text. I've filled in a lot for you already, but suffice to say that tokens are the preferred way to `gemsim` takes its input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_df=.5, \n",
    "                             min_df=8,\n",
    "                             max_features=1000,\n",
    "                             stop_words='english'\n",
    "                            )\n",
    "\n",
    "X_text = pd.DataFrame((vectorizer.fit_transform(samp_df.overview)>0).toarray())\n",
    "all_words = vectorizer.get_feature_names()\n",
    "d = {i : all_words[i] for i in range(len(all_words))}\n",
    "\n",
    "tokens = [[d[j] for j in X_text.columns[X_text.loc[i]].tolist()] for i in range(X_text.shape[0])]\n",
    "\n",
    "print(np.random.choice(tokens, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([len(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use `gensim`'s built in functionality to create a dictionary and bag of words for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train our model and output our results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = LdaModel(corpus, num_topics=10, id2word=dictionary, passes=30, random_state=random_state)\n",
    "topics = ldamodel.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see how our model classifies some of our favorite movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = titles['Superbad']\n",
    "\n",
    "print(samp_df.title.loc[i])\n",
    "print(samp_df.overview.loc[i])\n",
    "ldamodel.get_document_topics(corpus[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `pyLDAvis`, we can also build an interactive visualization of our topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_display = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
